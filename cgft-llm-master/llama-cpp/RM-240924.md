# Llama.cpp 内容补丁 （240924）

TODO: 更新中


由于llama.cpp经过版本变动，之前视频中提到的内容很多不再适用，因此再出一起更新视频。

## 0. 下载模型
安装用于测试的模型
```
export HF_ENDPOINT=https://hf-mirror.com

pip install huggingface_hub

# safetensors 
huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct --local-dir /root/autodl-fs/Qwen2.5-1.5B-Instruct

# gguf
# huggingface-cli 不能下载单独的一个文件
wget https://hf-mirror.com/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf
```


## 1.安装与编译
从本地安装
```
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# 启动gpu
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release

```
注意：在docker系统中，再安装docker会一些问题。
