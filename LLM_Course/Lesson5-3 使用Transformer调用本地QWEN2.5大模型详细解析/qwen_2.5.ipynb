{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我的版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) 封装定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangxincheng/miniconda3/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_name_or_path):\n",
    "    '''\n",
    "    加载模型\n",
    "    '''\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def chat_qwen(model, tokenizer, prompt):\n",
    "    '''\n",
    "    与模型进行交互\n",
    "    '''\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    # 使用分词器的 apply_chat_template 方法将消息格式化为模型可理解的输入格式\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,  # 别写错了，是tokenize不是tokenizer\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    #生成模型输出\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    # 由于模型输出包括输入模型，这里切去输入部分\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] \n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    # 将模型输出解码为文本\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name_or_path = \"../HF/Qwen2.5-7B-Instruct\"\n",
    "model, tokenizer = load_model(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 模型运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我叫Qwen，是由阿里云开发的超大规模语言模型。我的主要功能是生成与给定词语相关的高质量文本，能够回答问题、创作文字，比如写故事、写公文、写邮件、写剧本等等，还能表达观点，撰写代码。我会根据聊天的内容不断调整自己的回答，努力提供准确、有用的信息。如果你有任何问题或需要帮助，都可以随时告诉我哦！\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"你好，请你介绍一下自己。\"\n",
    "response = chat_qwen(model, tokenizer, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face 是一个致力于自然语言处理（NLP）的社区和平台。它主要提供了一个模型库，其中包含了许多预训练的深度学习模型，特别是用于文本数据的模型。这个库不仅包括了来自学术界和工业界的广泛模型，还提供了丰富的工具和资源，帮助研究人员和开发者更轻松地使用这些模型进行实验和开发。\n",
      "\n",
      "Hugging Face 还推出了几个重要的项目，比如 Transformers 库，这是一个非常流行的 Python 库，用于构建和使用基于 transformer 架构的机器学习模型。此外，Hugging Face 还创建了一个名为 Datasets 的库，该库提供了一套工具来加载、管理和预处理各种数据集，这对于训练机器学习模型至关重要。\n",
      "\n",
      "Hugging Face 社区也非常活跃，经常举办相关的研讨会、工作坊和技术分享活动，促进了 NLP 领域的知识交流和发展。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\n",
    "response = chat_qwen(model, tokenizer, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
